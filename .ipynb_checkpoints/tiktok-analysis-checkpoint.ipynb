{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5146e1-68e5-439e-918b-d012d0e48e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_recall_curve\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb5fa25-33d1-4e85-bfd5-fd51fc13cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\David\\PREDICT TIKTOK\\TikTok\\Your Activity\\Watch History.txt\"\n",
    "TARGET_TIMEZONE = 'Asia/Kuala_Lumpur'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f6bea7-745d-4b80-886b-6489d738b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(file_path):\n",
    "    \"\"\"\n",
    "    Parses the raw text file to extract timestamps.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "    # Regex to find Date: YYYY-MM-DD HH:MM:SS UTC\n",
    "    # We only care about the time, the link is irrelevant for the 'Clock'\n",
    "    regex_pattern = r\"Date:\\s*(\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}:\\d{2})\\s*UTC\"\n",
    "    matches = re.findall(regex_pattern, content)\n",
    "\n",
    "    if not matches:\n",
    "        print(\"No valid timestamps found.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(matches, columns=['timestamp'])\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Convert UTC to User Timezone\n",
    "    df['timestamp'] = df['timestamp'].dt.tz_localize('UTC').dt.tz_convert(TARGET_TIMEZONE)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f3c46a-2445-4a3b-9b06-d6448205ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_addiction_clock(df):\n",
    "    \"\"\"\n",
    "    Creates the Hour vs. Day heatmap.\n",
    "    \"\"\"\n",
    "    # Extract Hour and Day\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['day_of_week'] = df['timestamp'].dt.day_name()\n",
    "\n",
    "    # Order days logically, not alphabetically\n",
    "    days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    df['day_of_week'] = pd.Categorical(df['day_of_week'], categories=days_order, ordered=True)\n",
    "\n",
    "    # Create the Matrix (Pivot Table)\n",
    "    heatmap_data = df.groupby(['day_of_week', 'hour']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Reindex to ensure 24 hours and 7 days are always present (even if 0 activity)\n",
    "    heatmap_data = heatmap_data.reindex(days_order)\n",
    "    heatmap_data = heatmap_data.reindex(columns=range(24), fill_value=0)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(heatmap_data, cmap='magma', linewidths=.5, annot=False)\n",
    "    plt.title('The \"Addiction Clock\": Activity Heatmap of 26 weeks')\n",
    "    plt.xlabel('Hour of Day (0-23)')\n",
    "    plt.ylabel('Day of Week')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536985f7-6348-4261-9f41-fe8b7b1da63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = load_and_process_data(file_path)\n",
    "    if df is not None:\n",
    "        print(f\"Data Loaded: {len(df)} records found.\")\n",
    "        generate_addiction_clock(df)\n",
    "    else:\n",
    "        print(\"Could not load data. Check file path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2d3eaa-605e-492f-9486-bc645180664b",
   "metadata": {},
   "source": [
    "## MINUTES WATCHED IN 26 WEEKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c602bc2-ef90-4d56-a584-0ffef48e25df",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAP_THRESHOLD_MINUTES = 10\n",
    "DEFAULT_WATCH_SECONDS = 30 # Duration assumed if gap is too large\n",
    "\n",
    "def parse_and_process_history(file_path):\n",
    "    # 1. Load Data\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found.\")\n",
    "        return None\n",
    "\n",
    "    # 2. Extract Timestamps\n",
    "    regex_pattern = r\"Date:\\s*(\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}:\\d{2})\\s*UTC\"\n",
    "    matches = re.findall(regex_pattern, content)\n",
    "    \n",
    "    if not matches:\n",
    "        print(\"No valid data found.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(matches, columns=['timestamp'])\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # 3. Sort Ascending (Critical for Gap Calculation)\n",
    "    # The file is usually newest-first, so we must reverse it to calculate\n",
    "    # duration = next_start - current_start\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    # 4. Calculate Gaps (Duration)\n",
    "    # Shift(-1) lets us see the NEXT video's timestamp on the current row\n",
    "    df['next_timestamp'] = df['timestamp'].shift(-1)\n",
    "    \n",
    "    # Calculate difference in seconds\n",
    "    df['duration_seconds'] = (df['next_timestamp'] - df['timestamp']).dt.total_seconds()\n",
    "    \n",
    "    # 5. Apply \"Gap Theory\" Logic\n",
    "    # If gap is NaN (last video) OR gap > threshold, use default 30s\n",
    "    # Else, use the actual gap\n",
    "    threshold_seconds = GAP_THRESHOLD_MINUTES * 60\n",
    "    \n",
    "    def clean_duration(row):\n",
    "        if pd.isna(row['duration_seconds']):\n",
    "            return DEFAULT_WATCH_SECONDS\n",
    "        if row['duration_seconds'] > threshold_seconds:\n",
    "            return DEFAULT_WATCH_SECONDS\n",
    "        return row['duration_seconds']\n",
    "\n",
    "    df['minutes_watched'] = df.apply(clean_duration, axis=1) / 60\n",
    "    \n",
    "    # 6. Localize Timezone (for Day/Hour extraction)\n",
    "    df['local_time'] = df['timestamp'].dt.tz_localize('UTC').dt.tz_convert(TARGET_TIMEZONE)\n",
    "    df['hour'] = df['local_time'].dt.hour\n",
    "    df['day_of_week'] = df['local_time'].dt.day_name()\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8bdb91-1c61-4460-8516-8d7140b501c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_minutes_heatmap(df):\n",
    "    # Order days correctly\n",
    "    days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    df['day_of_week'] = pd.Categorical(df['day_of_week'], categories=days_order, ordered=True)\n",
    "    \n",
    "    # Sum minutes per Day/Hour\n",
    "    heatmap_data = df.groupby(['day_of_week', 'hour'])['minutes_watched'].sum().unstack(fill_value=0)\n",
    "    \n",
    "    # Reindex to ensure full 24x7 grid\n",
    "    heatmap_data = heatmap_data.reindex(days_order)\n",
    "    heatmap_data = heatmap_data.reindex(columns=range(24), fill_value=0)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(heatmap_data, cmap='magma', linewidths=0.5, linecolor='white', annot=False, fmt=\".0f\", cbar_kws={'label': 'Total Minutes Watched'})\n",
    "    \n",
    "    plt.title('True Usage: Minutes Watched')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Day of Week')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc59cca-53e7-440d-aaac-b6cc945a831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = parse_and_process_history(file_path)\n",
    "    if df is not None:\n",
    "        total_hours = df['minutes_watched'].sum() / 60\n",
    "        print(f\"Total Estimated Watch Time: {total_hours:.2f} hours\")\n",
    "        plot_minutes_heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3eda89-e76c-4ed9-b636-0144ffabfc95",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e86c9f-86ee-4b14-8102-a378e2e93503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_robust(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    matches = re.findall(r\"Date:\\s*(\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}:\\d{2})\\s*UTC\", content)\n",
    "    df = pd.DataFrame(matches, columns=['timestamp'])\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['local_time'] = df['timestamp'].dt.tz_localize('UTC').dt.tz_convert(TARGET_TIMEZONE)\n",
    "    df = df.sort_values('local_time').reset_index(drop=True)\n",
    "    \n",
    "    # Features\n",
    "    df['date'] = df['local_time'].dt.date\n",
    "    df['hour'] = df['local_time'].dt.hour\n",
    "    df['is_weekend'] = df['local_time'].dt.dayofweek >= 5\n",
    "    \n",
    "    # Sabotage Definitions\n",
    "    df['is_sleep_sabotage'] = df['hour'].isin([2, 3, 4, 5, 6, 7])\n",
    "    df['is_work_sabotage'] = (~df['is_weekend']) & (df['hour'].between(9, 18))\n",
    "    df['is_morning_trigger'] = df['hour'].isin([7, 8, 9, 10])\n",
    "\n",
    "    # Aggregation\n",
    "    daily = df.groupby('date').agg(\n",
    "        total_clicks=('timestamp', 'count'),\n",
    "        late_night_clicks=('is_sleep_sabotage', 'sum'),\n",
    "        work_hour_clicks=('is_work_sabotage', 'sum'),\n",
    "        morning_clicks=('is_morning_trigger', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    daily['day_of_week'] = pd.to_datetime(daily['date']).dt.dayofweek\n",
    "    \n",
    "    # Scoring Logic (Multipliers)\n",
    "    def calculate_score(row):\n",
    "        sleep_pen = row['late_night_clicks'] * 3.0\n",
    "        if row['day_of_week'] >= 5: # Weekend\n",
    "            return sleep_pen + (row['total_clicks'] * 0.2) \n",
    "        else: # Weekday\n",
    "            return sleep_pen + (row['work_hour_clicks'] * 2.0) + (row['total_clicks'] * 0.05)\n",
    "\n",
    "    daily['raw_score'] = daily.apply(calculate_score, axis=1)\n",
    "    \n",
    "    # *** THE FIX: SMOOTHING & OUTLIER CAPPING ***\n",
    "    \n",
    "    # 1. Cap Outliers (Winsorization at 95%)\n",
    "    # This ensures a random 10,000 score doesn't ruin the threshold\n",
    "    cap_value = daily['raw_score'].quantile(0.95)\n",
    "    daily['capped_score'] = np.where(daily['raw_score'] > cap_value, cap_value, daily['raw_score'])\n",
    "    \n",
    "    # 2. Exponential Smoothing (Span=3 days)\n",
    "    # This focuses on the HABIT, not the SPIKE.\n",
    "    daily['smoothed_score'] = daily['capped_score'].ewm(span=3).mean()\n",
    "    \n",
    "    # 3. Define \"General Bad Day\" Threshold\n",
    "    # We use the 70th percentile of the SMOOTHED data.\n",
    "    # This defines a \"Bad Phase\" rather than a random bad event.\n",
    "    ROBUST_THRESHOLD = daily['smoothed_score'].quantile(0.4)\n",
    "    daily['is_bad_habit'] = (daily['smoothed_score'] >= ROBUST_THRESHOLD).astype(int)\n",
    "    \n",
    "    print(f\"Outlier Cap Value: {cap_value:.1f}\")\n",
    "    print(f\"Robust Threshold (Smoothed): {ROBUST_THRESHOLD:.1f}\")\n",
    "    \n",
    "    # Feature Engineering\n",
    "    daily['prev_score'] = daily['smoothed_score'].shift(1)\n",
    "    daily['volatility'] = daily['smoothed_score'].rolling(window=5).std().shift(1)\n",
    "    daily['days_since_peak'] = 0 # Placeholder for advanced logic if needed\n",
    "    \n",
    "    daily_clean = daily.dropna().copy()\n",
    "    \n",
    "    # Select Features\n",
    "    features = ['day_of_week', 'prev_score', 'morning_clicks', 'volatility']\n",
    "    X = daily_clean[features]\n",
    "    y = daily_clean['is_bad_habit']\n",
    "    \n",
    "    return X, y, daily_clean, ROBUST_THRESHOLD\n",
    "\n",
    "# --- 2. EXECUTE PIPELINE ---\n",
    "X, y, daily_data, FINAL_THRESHOLD = build_dataset_robust(file_path)\n",
    "\n",
    "# Check Class Balance\n",
    "print(f\"Class Balance (Bad Habit %): {y.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aeec91-580a-4aaf-a97e-9d82395c5325",
   "metadata": {},
   "source": [
    "# EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe1bfb8-69cb-47d2-9a25-465af78ab591",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Distribution of the SMOOTHED Score\n",
    "# We plot 'smoothed_score' because that is what the model actually learns from.\n",
    "# It should look less jagged than the raw score.\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(daily_data['smoothed_score'], bins=20, kde=True, color='purple')\n",
    "plt.axvline(FINAL_THRESHOLD, color='red', linestyle='--', label=f'Threshold ({FINAL_THRESHOLD:.0f})')\n",
    "plt.title('Distribution of Robust Usage Scores (Smoothed)')\n",
    "plt.xlabel('Smoothed Habit Score')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Correlation Heatmap\n",
    "# Updated to use the new feature names (prev_score, morning_clicks, volatility)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "# We include 'late_night_clicks' to see how much it drives the score\n",
    "corr_cols = ['smoothed_score', 'prev_score', 'morning_clicks', 'volatility', 'late_night_clicks']\n",
    "sns.heatmap(daily_data[corr_cols].corr(), annot=True, cmap='magma', fmt=\".2f\")\n",
    "plt.title('Feature Correlation Matrix')\n",
    "\n",
    "\n",
    "# Plot 3: Day of Week Impact\n",
    "# Updated target variable to 'is_bad_habit'\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.barplot(x='day_of_week', y='is_bad_habit', data=daily_data, palette='magma', ci=None)\n",
    "plt.xticks(ticks=range(7), labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "plt.title('Probability of \"Bad Habit\" by Day')\n",
    "plt.ylabel('Risk Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d1943e-f40f-415b-a060-eebb86824811",
   "metadata": {},
   "source": [
    "# MODEL TRAINING AND VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a21ac0c-0998-42c7-a74a-2b0399b447e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. TRAIN ENSEMBLE ---\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "clf1 = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "clf2 = RandomForestClassifier(n_estimators=200, max_depth=5, class_weight='balanced', random_state=42)\n",
    "# Adjusted XGBoost for Smoothed Target\n",
    "clf3 = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.05, eval_metric='logloss', random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', clf1), ('rf', clf2), ('xgb', clf3)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355f755-d220-470e-ac39-9279d3786f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. EVALUATION ---\n",
    "y_proba = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Optimize Threshold\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "best_thresh = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "y_pred = (y_proba >= best_thresh).astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"FINAL ROBUST METRICS\")\n",
    "print(\"=\"*40)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Purples')\n",
    "plt.title('Confusion Matrix (Smoothed Target)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a02695-f49c-43f2-a7e7-7c51a0ac38b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(voting_clf, 'tiktok_voting_model.pkl')\n",
    "joblib.dump(scaler, 'tiktok_scaler.pkl')\n",
    "joblib.dump(FINAL_THRESHOLD, 'robust_threshold.pkl')\n",
    "joblib.dump(best_thresh, 'decision_threshold.pkl')\n",
    "print(\"Robust Model Saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda",
   "language": "python",
   "name": "conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
